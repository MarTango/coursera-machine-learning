#+TITLE: MACHINE LEARNING BY STANFORD UNIVERSITY (COURSERA)
#+AUTHOR: MARTIN TANG
#+LATEX_HEADER: \usepackage{amsmath}

* Week 1
** Intro
*** Supervised Learning
    In supervised learning, we are given a data set and already know
    what our correct output should look like, having the idea that
    there is a relationship between the input and the output.

    Supervised learning problems are categorized into "regression" and
    "classification" problems. In a regression problem, we are trying
    to predict results within a continuous output, meaning that we are
    trying to map input variables to some continuous function. In a
    classification problem, we are instead trying to predict results
    in a discrete output. In other words, we are trying to map input
    variables into discrete categories.
*** Unsupervised Learning
    Unsupervised learning allows us to approach problems with little
    or no idea what our results should look like. We can derive
    structure from data where we don't necessarily know the effect of
    the variables.

    We can derive this structure by clustering the data based on
    relationships among the variables in the data.

    With unsupervised learning there is no feedback based on the
    prediction results.
** Model & Cost Function
*** Model Representation
    We'll use $x^{(i)}$ to denote the "input" variables (features),
    and $y^{(i)}$ to denote the "output" or target variable that we
    are trying to predict. A pair $(x^{(i)}, y^{(i)})$ is called a
    training example, and the dataset we'll be using to learn - a list
    of $m$ training examples $(x^{(i)}, y^{(i)}); i=1,...,m$ - is called
    a training set. We will also use $X$ to denote the space of input
    variables and $Y$ to denote the space of output variables.

    To describe the supervised learning problem more formally, our
    goal is, given a training set, to learn a function $h: X
    \rightarrow Y$ so that $h$ (hypothesis) is a good predictor for
    the corresponding value of y.
*** Cost Function
    We can measure the accuracy of our hypothesis function by using a
    cost function. This takes an average difference of all the results
    of the hypothesis with inputs from $x$'s and the actual $y$'s

    \[
    J(\mathbf{\theta}) = \frac{1}{2m}\sum_{i=1}^m\big(h_\theta(x_i) - y_i\big)^2
    \]

    \[
    J(\mathbf{\theta}) = \frac{1}{2m}\big(\mathbf{h}_\theta(x) - \mathbf{y}\big)^2
    \]

** Parameter Learning
*** Gradient Descent
    Trying to minimise $J(\mathbf{\theta})$, so we use gradient descent:

    \[
    \mathbf{\theta}^{(j+1)} = \mathbf{\theta}^{(j)} - \alpha \frac{\partial}{\partial\theta} J(\mathbf{\theta^{(j)}})
    \]
    
    with $\alpha$ being our learning rate.
*** Gradient Descent for Linear Regression
    When specifically applied to the case of linear regression, we can 
    modify the equation to be:

    \[
    \mathbf{\theta}^{(j+1)} = \mathbf{\theta}^{(j)} - \frac{\alpha}{m} \big(\mathbf{h}_{\theta^{(j)}}(x) - \mathbf{y}\big) \frac{\partial}{\partial\theta} h_{\theta^{(j)}}(x)
    \]
    
* Week 2
** Multivariate Linear Regression
*** Multiple Features
    We now introduce notation for equations where we can have any
    number of input variables. (This is kind of backtracking but I'm
    gonna follow the course).
    
    - $x_j^{(i)}$ = value of feature $j$ in the $i^{\text{th}}$ training example
    - $x^{(i)}$ = the input (features) of the $i^{\text{th}}$ training example
    - $m$ = the number of training examples
    - $n$ = the number of features
      
    The multivariate form of the hypothesis function is:
    \[
    h_\theta(x) = \theta_0 + \theta_1x_1 + ... + \theta_nx_n
    \]
    
**** Notes
    $\hat{y} = X\theta$ where $X$ is $m \times n$, $\theta$ is $n \times 1$, $y$ is $m \times 1$.
*** Gradient Descent for Multiple Variables    
    \[
    \theta_j := \theta_j - \alpha \frac{1}{m} \sum_i\big(h_\theta(x^{(i)}) - y^{(i)}\big) \cdot x_j^{(i)}
    \]
**** Notes
     I think this is equivalent to
     \[
     \theta^{(j+1)} = \theta^{(j)} - \alpha \frac{1}{m} (X^{\top}X\theta - X^{\top}y)
     \]
*** Feature Scaling
    We can speed up gradient descent by having each of our input
    values in roughly the same range. This is because $\theta$ will
    descend quickly on small ranges and slowly on large ranges, and so
    will oscillate inefficiently down to the optimum when the
    variables are very uneven.
    
    The way to prevent this is to modify the ranges of our input
    variables so that they are all roughly the same. Ideally:

    \[
    -1 \leq x_{(i)} \leq 1
    \]

    or

    \[
    -0.5 \leq x_{(i)} \leq 0.5
    \]
    
    These aren't exact requirements; we're only trying to speed things
    up. The goal is to get all input variables into roughly one of
    these ranges, give or take a few.
    
    Two techniques to help with this are *feature scaling* and *mean
    normalisation*. Feature scaling involves dividing the input values
    by the range of the input variable, resulting in a new range of
    just 1. Mean normalisation involves subtracting the average values
    for an input variable from the values for that input variable
    resulting in a new average value for the input variable of just
    zero. To implement both of these techniques, adjust your input
    values as shown:

    \[
    x_i := \frac{x_i - \mu_i}{s_i}
    \]
    
    Where $\mu_i$ is the average of all the values for feature $(i)$
    and $s_i$ is the range of values $(\max - \min)_i$, or $s_i$ is
    the standard deviation.

    Note that dividing by the range, or dividig by the standard
    deviation give different results.

    For example, if $x_i$ represents housing prices with a range of
    100 to 2000 and a mean value of 1000, then:

    \[
    x_i := \frac{\text{price} - 1000}{1900}
    \]
*** Learning Rate
    If $\alpha$ is too small: slow convergence. If it's too large, may
    not decrease on every iteration and thus may not converge.
*** Features and Polynomial Regression
    We can improve our features and the form of our hypothesis
    function in a couple different ways.

    We can combine multiple features into one. For example, we can
    combine $x_1$ and $x_2$ into a new feature $x_3$ by taking $x_1
    \cdot x_2$.

**** Polynomial Regression
     We can change the curve of $h$ by making it a quadratic,cubic or
     square root function (or any other form). For example, if our
     hypothesis function is $h_\theta(x) = \theta_0 + \theta_1x_1$
     then we can create addition features based on $x_1$, to get
     $h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_1^2$ etc.
     
     So we have created new features $x_2 = x_1^2$ (or $x_3 = x_1^3$).
     
** Computing Parameters Analytically
*** Normal Equation
    We will minimise $J$ by explicitly taking its derivatives wrt the
    $\theta_j$'s and setting them to zero. This allows us to find the
    optimum theta without iteration.

    \[
    \theta = (X^\top X)^{-1}X^\top y
    \]
    
    | Gradient Descent             | Normal Equation                                              |
    |------------------------------+--------------------------------------------------------------|
    | Need to choose alpha         | No need to choose alpha                                      |
    | Needs many iterations        | No need to iterate                                           |
    | $(\mathcal{O}(kn^2)$         | $\mathcal{O}(n^3)$, need to calculate inverse of $X^{\top}X$ |
    | Works well when $n$ is large | Slow if $n$ is very large                                    |

    With the normal equation, computing the inversion has complexity
    $\mathcal{O}(n^3)$. So if we have a very large number of features,
    the normal equation will be slow. In practice, when $n$ exceeds
    10000 it might be a good time to go from a normal solution to an
    iterative process.
    
*** Normal Equation Noninvertibility
    When implementing the normal equation in octave we want to use the
    =pinv= function rather than =inv=. The =pinv= function will give
    you a value of $\theta$ even if $X^\top X$ is not invertible.

    Common causes might be having:
    - Linearly dependant features
    - Too many features ($m \leq n$). In this case, delete some
      features or use "regularisation".

* Week 3
** Classification and Representation
*** Classification
   The classification problem is just like the regression problem,
   except that the values we now want to predict take on only a small
   number of discrete values.

   For now we're going to assume that there are two categories: 0
   and 1.
*** Hypothesis Representation
   We could approach the classification problem ignoring the fact that
   y is discrete-valued, and use our linear regression algorithm to
   try to predict y given x. However, it is easy to construct examples
   where this method performs very poorly. Intuitively, it also
   doesn't make sense for $h_\theta(x)$ to take values large than 1 or
   smaller than 0 when we know that $y \in {0, 1}$. To fix this, let's
   change the form for our hypotheses $h_\theta(x)$ to satisfy $0 \leq
   h_\theta(x) \leq 1$. This is accomplished by plugging $\theta^\top
   x$ into the Logistic Function.

   Our new form uses the "Sigmoid Function" ("Logistic Function"):

   \[
   h_\theta(x) = g(\theta^\top x)
   \]
   \[
   g(z) = \frac{1}{1+e^{-z}}
   \]

   $h_\theta(x)$ will give us the probability that our output is 1.
*** Decision Boundary
   In order to get our discrete 0 or 1 classification, we can
   translate the output of the hypothesis function as follows:
   - $h_\theta(x) \leq 0.5 \Rightarrow y = 0$
   - $h_\theta(x) \geq 0.5 \Rightarrow y = 1$

   Looking at $g$, we can say that:
   \[
   y = \begin{cases}
   0,& \text{if } \theta^\top x < 0 \\
   1,& \text{if } \theta^\top x \geq 0
   \end{cases}
   \]
   
   The *decision boundary* is the line that separates the area where $y
   = 0$ and where $y = 1$. It is created by our hypothesis function.
** Logistic Regression Model
*** Cost Function
    We can't use the same cost function that we use for linear
    regression because the Logistic Function will cause the output to
    be wavy, causing many local optima. In other words, it will not be
    a convex function.

    Instead, our cost function looks like:
    
    \[
    J(\theta) = \frac{1}{m} \sum_{i=1}^m\text{Cost}\big(h_\theta(x^{(i)}), y^{(i)}\big)
    \]
    \[
    \text{Cost}(h_\theta(x), y) = \begin{cases}
    -\log(h_\theta(x)) & \text{if } y = 1 \\
    -\log(1 - h_\theta(x)) &  \text{if } y = 0
    \end{cases}
    \]

*** Simplified Cost Function and Gradient Descent
    We can combine the two conditional cases into:

    \[
    \text{Cost}\big(h_\theta(x), y\big) = -y \log ( h_\theta(x) ) - (1 - y) \log ( 1 - h_\theta(x) )
    \]

    And so we can write our entire cost function as:

    \[
    J(\theta) = - \frac{1}{m} \sum_{i=1}^m \big[ y^{(i)} \log (h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \big]
    \]

    A vectorised implementation is:

    \[
    h = g(X\theta)$
    \]
    \[
    J(\theta) = \frac{1}{m} \cdot \big( -y ^ {\top} \log(h) - (1 - y) ^ {\top} \log(1 - h)\big)
    \]

    And gradient descent becomes:

    \[
    \theta^{(i+1)} = \theta^{(i)} - \frac{a}{m} X ^ {\top} (g(X\theta^{(i)}) - y)
    \]

*** Advanced Optimisation
    'Conjugate Gradient', 'BFGS' and 'L-BFGS' are better, faster ways
    to optimise $\theta$ that can be used instead of gradient
    descent. Don't write these more sophisticated algorithms yourself
    but use the libraries instead.

    We first need to provide a function that evaluates the following
    functions for a given input value $\theta$:

    $J(\theta)$

    $\frac{\partial}{\partial\theta_j}J(\theta)$

    If we write a function that returns [J, gradient], we can use
    octave's =fminunc()= optimisation algorithm along with
    =optimset()= that creates an object containing the options we want
    to send to =fminunc()=.
    #+BEGIN_SRC octave :export code
      options = optimset('GradObj', 'on', 'MaxIter', 100);
      initialTheta = zeros(2, 1);
      [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
    #+END_SRC

    We give to the function =fminunc= our cost function, our initial
    vector of theta values, and the "options" object that we created
    beforehand.
    
** Multiclass Classification
   Now we approach the classification of data when we have more than
   two categories. Instead of $y = {0, 1}$ we will exapdn our
   definition so that $y = {0, 1, ..., n}$.

*** One-vs-all
    We divide our problem into n+1 binary classification problems; in
    each one, we predict the probability that $y$ is a member of one
    of our classes.

    \[ h_\theta^{(i)}(x) = P(y = i | x; \theta) \]
    
    Our prediction is $\max_i(h_\theta^{(i)}(x))$
** Solving Overfitting
*** The Problem of Overfitting
**** TL;DR
     Adding more features seems to give us better fits, but adding too
     many can result in a hypothesis function that fits the available
     data well but does not generalise well to predict new data.
*** Cost Function
    If we have overfitting from our hypoothesis function, we can
    reduce the weight that some of the terms in our function carry by
    increasing their cost.  Say we wanted to make $\theta_0 +
    \theta_1x \theta_2x^2 + \theta_3x^3 + \theta_4x^4$ more quadratic.
    We'll want to eliminate the influence of $\theta_3x^3$ and
    $\theta_4x^4$. Without actually getting rid of these features or
    changing the form of our hypothesis, we can instead modify our
    cost function:

    $\min_\theta\frac{1}{2m}\sum_{i=1}^m \big(h_\theta(x^{(i)}) -
    y^{(i)} \big)^2 + 1000\theta_3^2 + 1000\theta_4^2$
    
    We've added two extram terms that inflate the cost of $\theta_3$
    and $\theta_4$. Now, in oorder for the cost function to get cclose
    to zero, we will have to reduce the values of $\theta_3$ and
    $\theta_4$ to near zero. This will in turn greatly reduce the
    values of $\theta_3x^3$ and $\theta_4x^4$ in our hypothesis
    function.

    We could also regularise all of our theta parameters in a single
    summation as:

    $\min_\theta\frac{1}{2m}\sum_{i=1}^{m}\big(h_\theta(x^{(i)}) - y^{(i)}\big)^2 + \lambda\sum_{j=1}^n\theta_j^2$
    
    $\lambda$ is the *regularisation parameter*. It determines how
    much the costs of our theta parameters are inflated.

    If $\lambda$ is chosen to be too large, it may smooth out the
    function too much and cause underfitting.
*** Regularised Linear Regression
    We can apply regularisation too both linear regression and
    logistic regression.
**** Gradient Descent

     We will modify our gradient descent function to separate out
     $\theta_0$ from the rest of the parameters because we do not want
     to penalise $\theta_0$.

     $\theta_0 = \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x&{(i)}) - y^{(i)})x_0^{(i)}$

     $\theta_j = \theta_j - \alpha \Bigg[\Bigg( \sum_{i=1}^m(h_\theta(x&{(i)}) - y^{(i)})x_j^{(i)}\Bigg) + \frac{\lambda}{m}\theta_j\Bigg]$
     
     The term $\frac{\lambda}{m}\theta_j$ performs our
     regularisation. With some manipulation our update rule can be
     represented as:

     $\theta_j = \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$
**** Normal Equation
     
     To add in regularisation, the equation is the same as our
     original, except that we add another term inside the parentheses:
     
     $\theta = (X^{\top}X + \lambda\cdot L)^{-1}X^\top y$
     
     where $L = \begin{bmatrix}
                0 & & & & \\
                  & 1 & & & \\
                  & & 1 & & \\
                  & & & \ddots & \\
                  & & & & 1 \\
                \end{bmatrix}$

     L is a matrix with 0 at the top left and 1's down the diagonal,
     with 0's everywhere else. It should have dimension
     $(n+1)\times(n+1)$. Intuitively, this is the identity matrix
     multiplied with a single real number $\lambda$.

     Recall that if $m < n$, then $X^\top X$ is
     non-invertible. However, when we add the term $\lambda L$, then
     $X^\top X + \lambda L$ becomes invertible.
