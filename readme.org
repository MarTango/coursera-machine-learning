#+TITLE: MACHINE LEARNING BY STANFORD UNIVERSITY (COURSERA)
#+AUTHOR: MARTIN TANG
#+LATEX_HEADER: \usepackage{amsmath}

* Week 1
** Intro
*** Supervised Learning
    In supervised learning, we are given a data set and already know
    what our correct output should look like, having the idea that
    there is a relationship between the input and the output.

    Supervised learning problems are categorized into "regression" and
    "classification" problems. In a regression problem, we are trying
    to predict results within a continuous output, meaning that we are
    trying to map input variables to some continuous function. In a
    classification problem, we are instead trying to predict results
    in a discrete output. In other words, we are trying to map input
    variables into discrete categories.
*** Unsupervised Learning
    Unsupervised learning allows us to approach problems with little
    or no idea what our results should look like. We can derive
    structure from data where we don't necessarily know the effect of
    the variables.

    We can derive this structure by clustering the data based on
    relationships among the variables in the data.

    With unsupervised learning there is no feedback based on the
    prediction results.
** Model & Cost Function
*** Model Representation
    We'll use $x^{(i)}$ to denote the "input" variables (features),
    and $y^{(i)}$ to denote the "output" or target variable that we
    are trying to predict. A pair $(x^{(i)}, y^{(i)})$ is called a
    training example, and the dataset we'll be using to learn - a list
    of $m$ training examples $(x^{(i)}, y^{(i)}); i=1,...,m$ - is called
    a training set. We will also use $X$ to denote the space of input
    variables and $Y$ to denote the space of output variables.

    To describe the supervised learning problem more formally, our
    goal is, given a training set, to learn a function $h: X
    \rightarrow Y$ so that $h$ (hypothesis) is a good predictor for
    the corresponding value of y.
*** Cost Function
    We can measure the accuracy of our hypothesis function by using a
    cost function. This takes an average difference of all the results
    of the hypothesis with inputs from $x$'s and the actual $y$'s

    \[
    J(\mathbf{\theta}) = \frac{1}{2m}\sum_{i=1}^m\big(h_\theta(x_i) - y_i\big)^2
    \]

    \[
    J(\mathbf{\theta}) = \frac{1}{2m}\big(\mathbf{h}_\theta(x) - \mathbf{y}\big)^2
    \]

** Parameter Learning
*** Gradient Descent
    Trying to minimise $J(\mathbf{\theta})$, so we use gradient descent:

    \[
    \mathbf{\theta}^{(j+1)} = \mathbf{\theta}^{(j)} - \alpha \frac{\partial}{\partial\theta} J(\mathbf{\theta^{(j)}})
    \]
    
    with $\alpha$ being our learning rate.
*** Gradient Descent for Linear Regression
    When specifically applied to the case of linear regression, we can 
    modify the equation to be:

    \[
    \mathbf{\theta}^{(j+1)} = \mathbf{\theta}^{(j)} - \frac{\alpha}{m} \big(\mathbf{h}_{\theta^{(j)}}(x) - \mathbf{y}\big) \frac{\partial}{\partial\theta} h_{\theta^{(j)}}(x)
    \]
    
* Week 2
** Multivariate Linear Regression
*** Multiple Features
    We now introduce notation for equations where we can have any
    number of input variables. (This is kind of backtracking but I'm
    gonna follow the course).
    
    - $x_j^{(i)}$ = value of feature $j$ in the $i^{\text{th}}$ training example
    - $x^{(i)}$ = the input (features) of the $i^{\text{th}}$ training example
    - $m$ = the number of training examples
    - $n$ = the number of features
      
    The multivariate form of the hypothesis function is:
    \[
    h_\theta(x) = \theta_0 + \theta_1x_1 + ... + \theta_nx_n
    \]
    
**** Notes
    $\hat{y} = X\theta$ where $X$ is $m \times n$, $\theta$ is $n \times 1$, $y$ is $m \times 1$.
*** Gradient Descent for Multiple Variables    
    \[
    \theta_j := \theta_j - \alpha \frac{1}{m} \sum_i\big(h_\theta(x^{(i)}) - y^{(i)}\big) \cdot x_j^{(i)}
    \]
**** Notes
     I think this is equivalent to
     \[
     \theta^{(j+1)} = \theta^{(j)} - \alpha \frac{1}{m} (X^{\top}X\theta - X^{\top}y)
     \]
*** Feature Scaling
    We can speed up gradient descent by having each of our input
    values in roughly the same range. This is because $\theta$ will
    descend quickly on small ranges and slowly on large ranges, and so
    will oscillate inefficiently down to the optimum when the
    variables are very uneven.
    
    The way to prevent this is to modify the ranges of our input
    variables so that they are all roughly the same. Ideally:

    \[
    -1 \leq x_{(i)} \leq 1
    \]

    or

    \[
    -0.5 \leq x_{(i)} \leq 0.5
    \]
    
    These aren't exact requirements; we're only trying to speed things
    up. The goal is to get all input variables into roughly one of
    these ranges, give or take a few.
    
    Two techniques to help with this are *feature scaling* and *mean
    normalisation*. Feature scaling involves dividing the input values
    by the range of the input variable, resulting in a new range of
    just 1. Mean normalisation involves subtracting the average values
    for an input variable from the values for that input variable
    resulting in a new average value for the input variable of just
    zero. To implement both of these techniques, adjust your input
    values as shown:

    \[
    x_i := \frac{x_i - \mu_i}{s_i}
    \]
    
    Where $\mu_i$ is the average of all the values for feature $(i)$
    and $s_i$ is the range of values $(\max - \min)_i$, or $s_i$ is
    the standard deviation.

    Note that dividing by the range, or dividig by the standard
    deviation give different results.

    For example, if $x_i$ represents housing prices with a range of
    100 to 2000 and a mean value of 1000, then:

    \[
    x_i := \frac{\text{price} - 1000}{1900}
    \]
*** Learning Rate
    If $\alpha$ is too small: slow convergence. If it's too large, may
    not decrease on every iteration and thus may not converge.
*** Features and Polynomial Regression
    We can improve our features and the form of our hypothesis
    function in a couple different ways.

    We can combine multiple features into one. For example, we can
    combine $x_1$ and $x_2$ into a new feature $x_3$ by taking $x_1
    \cdot x_2$.

**** Polynomial Regression
     We can change the curve of $h$ by making it a quadratic,cubic or
     square root function (or any other form). For example, if our
     hypothesis function is $h_\theta(x) = \theta_0 + \theta_1x_1$
     then we can create addition features based on $x_1$, to get
     $h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_1^2$ etc.
     
     So we have created new features $x_2 = x_1^2$ (or $x_3 = x_1^3$).
     
** Computing Parameters Analytically
*** Normal Equation
    We will minimise $J$ by explicitly taking its derivatives wrt the
    $\theta_j$'s and setting them to zero. This allows us to find the
    optimum theta without iteration.

    \[
    \theta = (X^\top X)^{-1}X^\top y
    \]
    
    | Gradient Descent             | Normal Equation                                              |
    |------------------------------+--------------------------------------------------------------|
    | Need to choose alpha         | No need to choose alpha                                      |
    | Needs many iterations        | No need to iterate                                           |
    | $(\mathcal{O}(kn^2)$         | $\mathcal{O}(n^3)$, need to calculate inverse of $X^{\top}X$ |
    | Works well when $n$ is large | Slow if $n$ is very large                                    |

    With the normal equation, computing the inversion has complexity
    $\mathcal{O}(n^3)$. So if we have a very large number of features,
    the normal equation will be slow. In practice, when $n$ exceeds
    10000 it might be a good time to go from a normal solution to an
    iterative process.
    
*** Normal Equation Noninvertibility
    When implementing the normal equation in octave we want to use the
    =pinv= function rather than =inv=. The =pinv= function will give
    you a value of $\theta$ even if $X^\top X$ is not invertible.

    Common causes might be having:
    - Linearly dependant features
    - Too many features ($m \leq n$). In this case, delete some
      features or use "regularisation".

* Week 3
** Classification and Representation
*** Classification
   The classification problem is just like the regression problem,
   except that the values we now want to predict take on only a small
   number of discrete values.

   For now we're going to assume that there are two categories: 0
   and 1.
*** Hypothesis Representation
   We could approach the classification problem ignoring the fact that
   y is discrete-valued, and use our linear regression algorithm to
   try to predict y given x. However, it is easy to construct examples
   where this method performs very poorly. Intuitively, it also
   doesn't make sense for $h_\theta(x)$ to take values large than 1 or
   smaller than 0 when we know that $y \in {0, 1}$. To fix this, let's
   change the form for our hypotheses $h_\theta(x)$ to satisfy $0 \leq
   h_\theta(x) \leq 1$. This is accomplished by plugging $\theta^\top
   x$ into the Logistic Function.

   Our new form uses the "Sigmoid Function" ("Logistic Function"):

   \[
   h_\theta(x) = g(\theta^\top x)
   \]
   \[
   z = \theta^\top x
   \]
   \[
   g(z) = \frac{1}{1+e^{-z}}
   \]

   $h_\theta(x)$ will give us the probability that our output is 1.
*** Decision Boundary
   In order to get our discrete 0 or 1 classification, we can
   translate the output of the hypothesis function as follows:
   - $h_\theta(x) \leq 0.5 \Rightarrow y = 0$
   - $h_\theta(x) \geq 0.5 \Rightarrow y = 1$

   Looking at $g$, we can say that:
   \[
   y = \begin{cases}
   0,& \text{if } \theta^\top x < 0 \\
   1,& \text{if } \theta^\top x \geq 0
   \end{cases}
   \]
   
   The *decision boundary* is the line that separates the area where $y
   = 0$ and where $y = 1$. It is created by our hypothesis function.
** Logistic Regression Model
*** Cost Function
    We can't use the same cost function that we use for linear
    regression because the Logistic Function will cause the output to
    be wavy, causing many local optima. In other words, it will not be
    a convex function.

    Instead, our cost function looks like:
    
    \[
    J(\theta) = \frac{1}{m} \sum_{i=1}^m\text{Cost}\big(h_\theta(x^{(i)}), y^{(i)}\big)
    \]
    \[
    \text{Cost}(h_\theta(x), y) = \begin{cases}
    -\log(h_\theta(x)) & \text{if } y = 1 \\
    -\log(1 - h_\theta(x)) &  \text{if } y = 0
    \end{cases}
    \]
